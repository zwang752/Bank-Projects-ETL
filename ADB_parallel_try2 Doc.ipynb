{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_url = 'https://www.adb.org/projects'\n",
    "file_name='ADB_2'\n",
    "\n",
    "#create a document to save downloaded pdf\n",
    "try:\n",
    "    os.mkdir(file_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "path=r'C:\\Users\\yzhhu\\Desktop\\G.A.I_summer\\bank grabber\\{}'.format(file_name)\n",
    "web='https://www.adb.org'\n",
    "hh='?page='\n",
    "page_range=list(range(507))\n",
    "\n",
    "#You will need this step to create a public list that each processor can manipulate\n",
    "manager = multiprocessing.Manager()\n",
    "projectlist = manager.list()\n",
    "status_list=manager.list()\n",
    "status_fail=manager.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pdf_include(url,name,status):\n",
    "    opener=urllib.request.build_opener()\n",
    "    opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]\n",
    "    urllib.request.install_opener(opener)\n",
    "    os.chdir(path)\n",
    "    try:\n",
    "        os.mkdir(path+'/'+status)\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(path+'/'+status)\n",
    "    urllib.request.urlretrieve(url,name+'.pdf')\n",
    "    print('save {} successfully'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all urls\n",
    "def get_url (page):\n",
    "    linklist=[]\n",
    "    projectlist=[]\n",
    "    for i in page:\n",
    "        linklist.append(my_url+hh+str(i))\n",
    "    return linklist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get project with page number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linklist=get_url(page_range)\n",
    "projectlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get project urls\n",
    "def get_project (link):\n",
    "    global projectlist\n",
    "    r=requests.get(link)\n",
    "    page_soup = soup(r.content, \"html.parser\")\n",
    "    links = [a['href'] for a in page_soup.find_all('a', href=True)]\n",
    "    for k in links:\n",
    "        if ('/projects/' in k) and ('/main' in k):\n",
    "            projectlist.append(k)\n",
    "\n",
    "    print('finished {}'.format(link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get project link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use multiprocessing to repeatedly do the same function(Here is the formula)\n",
    "\n",
    "#Returns how many processors are in your computer\n",
    "num_cores=multiprocessing.cpu_count()\n",
    "Parallel(n_jobs=num_cores)(delayed(get_project)(j) for j in linklist)\n",
    "#We have the project list now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get all the project names and urls\n",
    "name_list=list(map(lambda x:x.split('projects/')[1].split('/main')[0],projectlist))\n",
    "project_final_list=list(map(lambda x: web+x+'#project-pds',projectlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(project_final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# downloading...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the 'status' of each project and seperatly download them into different folders\n",
    "def get_everything (link,name):\n",
    "    #Call the public list you have created in the function you will put into multiprocessing\n",
    "    global status_fail\n",
    "    r=requests.get(link)\n",
    "    ini_soup = soup(r.content, \"html.parser\")\n",
    "    try:\n",
    "        status=ini_soup.find_all('span',{'class':['project-status Proposed']})[0].string.split(' ')[1]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        status=ini_soup.find_all('span',{'class':['project-status Dropped / Terminated']})[0].string.split(' ')[1]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        status=ini_soup.find_all('span',{'class':['project-status Active']})[0].string.split(' ')[1]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        status=ini_soup.find_all('span',{'class':['project-status Approved']})[0].string.split(' ')[1]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        status=ini_soup.find_all('span',{'class':['project-status Closed']})[0].string.split(' ')[1]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        status=ini_soup.find_all('span',{'class':['project-status Archived']})[0].string.split(' ')[1]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        save_pdf_include(link, name, status)\n",
    "        print('successfully append {}'.format(link))\n",
    "      \n",
    "    #For those project pdfs downloaded unsuccessfully we put them into another list\n",
    "    except:\n",
    "        status_fail.append(link)\n",
    "        print('fking messed up with {}'.format(link))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time count\n",
    "start=time.time()\n",
    "#Returns how many processors are in your computer\n",
    "num_cores=multiprocessing.cpu_count()\n",
    "Parallel(n_jobs=num_cores)(delayed(get_everything)(j,p) for j,p in zip(project_final_list,name_list))\n",
    "print('------time used: {} ------------'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
